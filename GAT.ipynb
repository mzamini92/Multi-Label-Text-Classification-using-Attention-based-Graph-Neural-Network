{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a99fc3",
   "metadata": {},
   "source": [
    "# Start of Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7ddd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df=pd.read_csv('trainn.csv')\n",
    "test_df=pd.read_csv('testt.csv')\n",
    "data=pd.read_csv('output.csv')\n",
    "from ast import literal_eval\n",
    "\n",
    "train_df[\"terms\"] = train_df[\"terms\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")\n",
    "from ast import literal_eval\n",
    "\n",
    "test_df[\"terms\"] = test_df[\"terms\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")\n",
    "from ast import literal_eval\n",
    "\n",
    "data[\"terms\"] = data[\"terms\"].apply(\n",
    "    lambda x: literal_eval(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5190b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462af254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs.CV\n",
      "cs.LG\n",
      "eess.IV\n",
      "cs.MA\n",
      "stat.ML\n",
      "cs.CR\n",
      "cs.SI\n",
      "cs.AI\n",
      "cs.RO\n",
      "cs.NE\n",
      "cs.IR\n",
      "cs.CY\n",
      "cs.MM\n",
      "cs.SY\n",
      "eess.SY\n",
      "cs.CG\n",
      "cs.CL\n",
      "eess.SP\n",
      "cs.IT\n",
      "math.IT\n",
      "math.PR\n",
      "cs.DS\n",
      "physics.chem-ph\n",
      "cs.SD\n",
      "eess.AS\n",
      "cs.GR\n",
      "stat.AP\n",
      "cs.PL\n",
      "cs.DB\n",
      "math.OC\n",
      "cs.HC\n",
      "cs.GT\n",
      "math.ST\n",
      "stat.TH\n",
      "cs.DM\n",
      "math.CO\n",
      "stat.ME\n",
      "physics.comp-ph\n",
      "physics.ao-ph\n",
      "cs.NI\n",
      "math-ph\n",
      "math.MP\n",
      "cs.NA\n",
      "math.NA\n",
      "cs.SE\n",
      "cs.LO\n",
      "cs.DC\n",
      "math.CA\n",
      "physics.optics\n",
      "stat.CO\n",
      "cs.CE\n",
      "stat.OT\n",
      "cs.AR\n",
      "cs.ET\n",
      "physics.soc-ph\n",
      "math.AT\n",
      "physics.flu-dyn\n",
      "physics.data-an\n",
      "physics.geo-ph\n",
      "physics.app-ph\n",
      "physics.ins-det\n",
      "cond-mat.stat-mech\n",
      "cs.CC\n",
      "math.DS\n",
      "cs.SC\n",
      "physics.class-ph\n",
      "physics.med-ph\n",
      "cs.PF\n",
      "math.RT\n",
      "05C60\n",
      "math.CV\n",
      "cs.DL\n",
      "math.SP\n",
      "math.AP\n",
      "C.1.3\n",
      "cs.MS\n",
      "physics.bio-ph\n",
      "math.MG\n",
      "math.GR\n",
      "physics.plasm-ph\n",
      "cs.FL\n",
      "math.RA\n",
      "physics.ed-ph\n",
      "physics.pop-ph\n",
      "math.DG\n",
      "math.OA\n",
      "I.5\n",
      "math.FA\n",
      "math.NT\n",
      "math.AC\n",
      "math.AG\n",
      "math.CT\n",
      "math.LO\n",
      "math.HO\n",
      "H.0\n",
      "physics.acc-ph\n",
      "I.2.2\n",
      "68W50\n"
     ]
    }
   ],
   "source": [
    "unique_categories=[]\n",
    "for i in range(len(train_df.terms)):\n",
    "    for j in range(len(train_df.terms[i])):\n",
    "        if train_df.terms[i][j] not in unique_categories:\n",
    "            unique_categories.append(train_df.terms[i][j])\n",
    "            print(train_df.terms[i][j])\n",
    "        else:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209eb8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928246b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['05C60', '68W50', 'C.1.3', 'G.3', 'H.0', 'H.3.3', 'I.2.2', 'I.5',\n",
       "       'cond-mat.stat-mech', 'cs.AI', 'cs.AR', 'cs.CC', 'cs.CE', 'cs.CG',\n",
       "       'cs.CL', 'cs.CR', 'cs.CV', 'cs.CY', 'cs.DB', 'cs.DC', 'cs.DL',\n",
       "       'cs.DM', 'cs.DS', 'cs.ET', 'cs.FL', 'cs.GR', 'cs.GT', 'cs.HC',\n",
       "       'cs.IR', 'cs.IT', 'cs.LG', 'cs.LO', 'cs.MA', 'cs.MM', 'cs.MS',\n",
       "       'cs.NA', 'cs.NE', 'cs.NI', 'cs.OS', 'cs.PF', 'cs.PL', 'cs.RO',\n",
       "       'cs.SC', 'cs.SD', 'cs.SE', 'cs.SI', 'cs.SY', 'eess.AS', 'eess.IV',\n",
       "       'eess.SP', 'eess.SY', 'math-ph', 'math.AC', 'math.AG', 'math.AP',\n",
       "       'math.AT', 'math.CA', 'math.CO', 'math.CT', 'math.CV', 'math.DG',\n",
       "       'math.DS', 'math.FA', 'math.GR', 'math.GT', 'math.HO', 'math.IT',\n",
       "       'math.LO', 'math.MG', 'math.MP', 'math.NA', 'math.NT', 'math.OA',\n",
       "       'math.OC', 'math.PR', 'math.RA', 'math.RT', 'math.SP', 'math.ST',\n",
       "       'physics.acc-ph', 'physics.ao-ph', 'physics.app-ph',\n",
       "       'physics.bio-ph', 'physics.chem-ph', 'physics.class-ph',\n",
       "       'physics.comp-ph', 'physics.data-an', 'physics.ed-ph',\n",
       "       'physics.flu-dyn', 'physics.geo-ph', 'physics.ins-det',\n",
       "       'physics.med-ph', 'physics.optics', 'physics.plasm-ph',\n",
       "       'physics.pop-ph', 'physics.soc-ph', 'stat.AP', 'stat.CO',\n",
       "       'stat.ME', 'stat.ML', 'stat.OT', 'stat.TH'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform(data.terms)\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4adb762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarized = []\n",
    "for i in range(len(train_df.terms)):\n",
    "#     print(i)\n",
    "    label_binarized.append(mlb.transform([train_df[\"terms\"].iloc[i]]))\n",
    "    \n",
    "sss=[]\n",
    "for i in label_binarized:\n",
    "    sss.append(i[0])\n",
    "\n",
    "train_df['binarized'] = sss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4affd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarized = []\n",
    "for i in range(len(test_df.terms)):\n",
    "#     print(i)\n",
    "    label_binarized.append(mlb.transform([test_df[\"terms\"].iloc[i]]))\n",
    "    \n",
    "sss=[]\n",
    "for i in label_binarized:\n",
    "    sss.append(i[0])\n",
    "\n",
    "test_df['binarized'] = sss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adddcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp = train_df.drop(['terms'], axis=1)\n",
    "X_test_temp = test_df.drop(['terms'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f4d0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_temp = train_df.terms\n",
    "y_test_temp = test_df.terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad759a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>summaries</th>\n",
       "      <th>binarized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27230</td>\n",
       "      <td>The 3D pose estimation from a single image is ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7366</td>\n",
       "      <td>Since the generative neural networks have made...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42237</td>\n",
       "      <td>Action recognition has been a widely studied t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34674</td>\n",
       "      <td>Deep reinforcement learning has achieved great...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10317</td>\n",
       "      <td>The training process of neural networks usuall...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34683</th>\n",
       "      <td>28745</td>\n",
       "      <td>The present Multi-view stereo (MVS) methods wi...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34684</th>\n",
       "      <td>6170</td>\n",
       "      <td>We present a novel modular object detection co...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34685</th>\n",
       "      <td>31441</td>\n",
       "      <td>We present a new practical framework based on ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34686</th>\n",
       "      <td>17681</td>\n",
       "      <td>A low precision deep neural network training t...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34687</th>\n",
       "      <td>23346</td>\n",
       "      <td>Multi-agent interacting systems are prevalent ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34688 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                          summaries  \\\n",
       "0           27230  The 3D pose estimation from a single image is ...   \n",
       "1            7366  Since the generative neural networks have made...   \n",
       "2           42237  Action recognition has been a widely studied t...   \n",
       "3           34674  Deep reinforcement learning has achieved great...   \n",
       "4           10317  The training process of neural networks usuall...   \n",
       "...           ...                                                ...   \n",
       "34683       28745  The present Multi-view stereo (MVS) methods wi...   \n",
       "34684        6170  We present a novel modular object detection co...   \n",
       "34685       31441  We present a new practical framework based on ...   \n",
       "34686       17681  A low precision deep neural network training t...   \n",
       "34687       23346  Multi-agent interacting systems are prevalent ...   \n",
       "\n",
       "                                               binarized  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "34683  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "34684  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "34685  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "34686  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "34687  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[34688 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3824a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ssl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ssl/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2022-12-13 18:00:09.487852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-13 18:00:10.087418: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:\n",
      "2022-12-13 18:00:10.087473: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.2/lib64:\n",
      "2022-12-13 18:00:10.087478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ssl/miniconda3/envs/cuda_tf2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "# nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# from models import MAGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de1372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label shape torch.Size([34688, 102])\n",
      "Test label shape torch.Size([17086, 102])\n"
     ]
    }
   ],
   "source": [
    "text_train = X_train_temp.summaries.values\n",
    "text_test = X_test_temp.summaries.values\n",
    "\n",
    "y_train = torch.from_numpy(np.vstack(X_train_temp.binarized.values)).float()\n",
    "y_test = torch.from_numpy(np.vstack(X_test_temp.binarized.values)).float()\n",
    "\n",
    "print('Train label shape {}'.format(y_train.shape))\n",
    "print('Test label shape {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fdc859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b01d2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp.binarized\n",
    "df_train=pd.DataFrame({'summaries':text_train,'terms':y_train_temp,'binarized':X_train_temp.binarized.values})\n",
    "df_test=pd.DataFrame({'summaries':text_test,'terms':y_test_temp,'binarized':X_test_temp.binarized.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90b0ea45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17086"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53dd8de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51774"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ffd53a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "  def __init__(self, inp, out, slope):\n",
    "    super(GraphAttentionLayer, self).__init__()\n",
    "    self.W = nn.Linear(inp, out, bias=False)\n",
    "    self.a = nn.Linear(out*2, 1, bias=False)\n",
    "    self.leakyrelu = nn.LeakyReLU(slope)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, h, adj):\n",
    "    Wh = self.W(h)\n",
    "    Whcat = self.Wh_concat(Wh, adj)\n",
    "    e = self.leakyrelu(self.a(Whcat).squeeze(2))\n",
    "    zero_vec = -9e15*torch.ones_like(e)\n",
    "    attention = torch.where(adj > 0, e, zero_vec)\n",
    "    attention = self.softmax(attention)\n",
    "    h_hat = torch.mm(attention, Wh)\n",
    "\n",
    "    return h_hat\n",
    " \n",
    "  def Wh_concat(self, Wh, adj):\n",
    "    N = Wh.size(0)\n",
    "    Whi = Wh.repeat_interleave(N, dim=0)\n",
    "    Whj = Wh.repeat(N, 1)\n",
    "    WhiWhj = torch.cat([Whi, Whj], dim=1)\n",
    "    WhiWhj = WhiWhj.view(N, N, Wh.size(1)*2)\n",
    "\n",
    "    return WhiWhj\n",
    " \n",
    "class MultiHeadGAT(nn.Module):\n",
    "  def __init__(self, inp, out, heads, slope):\n",
    "    super(MultiHeadGAT, self).__init__()\n",
    "    self.attentions = nn.ModuleList([GraphAttentionLayer(inp, out, slope) for _ in range(heads)])\n",
    "    self.tanh = nn.Tanh()\n",
    "  \n",
    "  def forward(self, h, adj):\n",
    "    heads_out = [att(h, adj) for att in self.attentions]\n",
    "    out = torch.stack(heads_out, dim=0).mean(0)\n",
    "    \n",
    "    return self.tanh(out)\n",
    " \n",
    "class GAT(nn.Module):\n",
    "  def __init__(self, inp, out, heads, slope=0.01):\n",
    "    super(GAT, self).__init__()\n",
    "    self.gat1 = MultiHeadGAT(inp, out, heads, slope)\n",
    "    self.gat2 = MultiHeadGAT(out, out, heads, slope)\n",
    "  \n",
    "  def forward(self, h, adj):\n",
    "    out = self.gat1(h, adj)\n",
    "    out = self.gat2(out, adj)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Using static embedding\n",
    "class MAGNET(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, adjacency, embeddings, heads=6, slope=0.05, dropout=0.4):\n",
    "        super(MAGNET, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.rnn = nn.LSTM(input_size,\n",
    "                           hidden_size,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=True)\n",
    "        self.gat = GAT(input_size, hidden_size*2, heads, slope)\n",
    "        \n",
    "        self.adjacency = nn.Parameter(adjacency)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "#         self.predicted_values = None\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, token, label_embedding):\n",
    "        features = self.embedding(token)\n",
    "        \n",
    "        out, (hidden, cell) = self.rnn(features)\n",
    "        \n",
    "        out = torch.cat([hidden[-2, :, :], hidden[-1, :, :]], dim=1)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        att = self.dropout(self.gat(label_embedding, self.adjacency))\n",
    "        att = att.transpose(0, 1)\n",
    "        \n",
    "        out = torch.mm(out, att)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def predict(self, token, label_embedding):\n",
    "        # run the input data through the model\n",
    "        output = self.forward(token, label_embedding)\n",
    "        # apply a sigmoid function to the output to get the probabilities\n",
    "        probabilities = torch.sigmoid(output)\n",
    "        # round the probabilities to get the final predictions\n",
    "        predictions = probabilities.round()\n",
    "        return predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b024cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contextual Embedding\n",
    "\n",
    "class ContextMAGNET(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, adjacency, heads=8, slope=0.01, dropout=0.3):\n",
    "    super(ContextMAGNET, self).__init__()\n",
    "\n",
    "    self.rnn = nn.LSTM(input_size,\n",
    "                        hidden_size,\n",
    "                        batch_first=True,\n",
    "                        bidirectional=True)\n",
    "\n",
    "    self.gat = GAT(input_size, hidden_size*2, heads, slope)\n",
    "    \n",
    "    self.adjacency = nn.Parameter(adjacency)\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "  def forward(self, features, label_embedding):\n",
    "\n",
    "    out, (hidden, cell) = self.rnn(features)\n",
    "    \n",
    "    out = torch.cat([hidden[-2, :, :], hidden[-1, :, :]], dim=1)\n",
    "    out = self.dropout(out)\n",
    "    \n",
    "    att = self.dropout(self.gat(label_embedding, self.adjacency))\n",
    "    att = att.transpose(0, 1)\n",
    "    \n",
    "    out = torch.mm(out, att)\n",
    " \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8416558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build adjacency matrix based on Co-Occurencies label\n",
    "def buildAdjacencyCOOC(data_label):\n",
    "  adj = data_label.T.dot(data_label).astype('float')\n",
    "  for i in range(len(adj)):\n",
    "    adj[i] = adj[i] / adj[i,i]\n",
    "  \n",
    "  return torch.from_numpy(adj.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78ea913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.init import kaiming_normal_, xavier_normal_\n",
    "\n",
    "def buildAdjacencyCOOC(data_label):\n",
    "    # Transpose the data_label matrix\n",
    "    adj = data_label.T\n",
    "    \n",
    "    # Calculate the dot product of the transposed matrix and the original matrix\n",
    "    adj = adj.dot(data_label)\n",
    "    \n",
    "    # Convert the adjacency matrix to a PyTorch tensor\n",
    "    adj = torch.from_numpy(adj.astype('float32'))\n",
    "    \n",
    "    # Initialize the weights of the adjacency matrix using Kaiming (He) initialization\n",
    "    kaiming_normal_(adj)\n",
    "    #   xavier_normal_(adj)\n",
    "    \n",
    "    # Normalize the matrix by dividing each row by its diagonal element\n",
    "    for i in range(len(adj)):\n",
    "        adj[i] = adj[i] / adj[i,i]\n",
    "        \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "756fc2a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -1.2566, -7.8096,  ...,  1.6039,  5.1265,  2.6434],\n",
      "        [ 0.9806,  1.0000, -1.7846,  ...,  3.7889, -8.1096,  2.4497],\n",
      "        [ 1.9922, -0.6105,  1.0000,  ..., -1.2487, -0.9279, -0.4661],\n",
      "        ...,\n",
      "        [ 0.0894,  0.3875,  0.5331,  ...,  1.0000,  0.3280,  0.8170],\n",
      "        [-1.1974,  0.0955, -0.3260,  ..., -1.1946,  1.0000,  1.2786],\n",
      "        [ 2.0677,  0.0190, -0.5569,  ..., -1.0211,  0.3364,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "adjacency = buildAdjacencyCOOC(y_train.numpy())\n",
    "print(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff588084",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Text cleaning function\n",
    "def preprocessingText(text, stop=stop):\n",
    "  text = text.lower() #text to lowercase\n",
    "  text = re.sub(r'&lt;', '', text) #remove '&lt;' tag\n",
    "  text = re.sub(r'<.*?>', '', text) #remove html\n",
    "  text = re.sub(r'[0-9]+', '', text) #remove number\n",
    "  text = \" \".join([word for word in text.split() if word not in stop]) #remove stopwords\n",
    "  text = re.sub(r'[^\\w\\s]', '', text) #remove punctiation\n",
    "  text = re.sub(r'[^\\x00-\\x7f]', '', text) #remove non ASCII strings\n",
    "  for c in ['\\r', '\\n', '\\t'] :\n",
    "    text = re.sub(c, ' ', text) #replace newline and tab with tabs\n",
    "  text = re.sub('\\s+', ' ', text) #replace multiple spaces with one space\n",
    "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "  return text\n",
    "\n",
    "#Load Word Representation Vector\n",
    "def loadWRVModel(File):\n",
    "    print(\"Loading Word Representation Vector Model\")\n",
    "    f = open(File,'r')\n",
    "    WRVModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        try:\n",
    "          wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        except:\n",
    "          print(splitLines[1:])\n",
    "          print(len(splitLines[1:]))\n",
    "          break\n",
    "        WRVModel[word] = wordEmbedding\n",
    "    print(len(WRVModel),\" words loaded!\")\n",
    "    return WRVModel\n",
    "\n",
    "\n",
    "def check_accuracy(model, label_embedding, X, y):\n",
    "  \n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    out = model(X, label_embedding)\n",
    "    y_pred = torch.sigmoid(out.detach()).round().cpu()\n",
    "    f1score = f1_score(y, y_pred, average='micro')\n",
    "    hammingloss = hamming_loss(y, y_pred)\n",
    "  \n",
    "  return hammingloss, f1score\n",
    "\n",
    "def train(model,\n",
    "          X_train,\n",
    "          X_test,\n",
    "          label_embedding,\n",
    "          y_train,\n",
    "          y_test,\n",
    "          total_epoch=50,\n",
    "          batch_size=250,\n",
    "          learning_rate=0.001,\n",
    "          save_path='./model.pt',\n",
    "          state=None):\n",
    "  \n",
    "  device = torch.device(\"cpu\")#\"cuda\" if torch.cuda.is_available() else \n",
    "\n",
    "  train_data = DataLoader(dataset(X_train, y_train), batch_size=batch_size)\n",
    "  X_test = X_test.to(device)\n",
    "\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "  label_embedding= label_embedding.to(device)\n",
    "\n",
    "  if state:\n",
    "    state = torch.load(state)\n",
    "    model = model.load_state_dict(state['last_model'])\n",
    "    optimizer = optimizer.load_state_dict(state['optimizer'])\n",
    "  \n",
    "  else:\n",
    "    model = model.to(device)\n",
    "    state = dict()\n",
    "    state['microf1'] = []\n",
    "    state['hammingloss'] = []\n",
    "    state['val_hammingloss'] = []\n",
    "    state['val_microf1'] = []\n",
    "    state['epoch_time'] = []\n",
    "    \n",
    "  epoch = 1\n",
    "  \n",
    "  best_train = 0\n",
    "  best_val = 0\n",
    "  \n",
    "  while epoch <= total_epoch:\n",
    "    running_loss = 0\n",
    "    y_pred = []\n",
    "    epoch_time = 0\n",
    "    model.train()\n",
    "    for index, (X, y) in enumerate(train_data):\n",
    "      \n",
    "      t = time.time()\n",
    "\n",
    "      #forward\n",
    "      out = model(X.to(device), label_embedding)\n",
    "      loss = criterion(out, y.to(device))\n",
    "\n",
    "      #backward\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "      #update\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_time += time.time() - t\n",
    "      y_pred.append(torch.sigmoid(out.detach()).round().cpu())\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    y_pred = torch.vstack(y_pred)\n",
    "    f1score = f1_score(y_train, y_pred, average='micro')\n",
    "    hammingloss = hamming_loss(y_train, y_pred)\n",
    "    val_hamming, val_f1score = check_accuracy(model, label_embedding, X_test, y_test)\n",
    "\n",
    "    state['microf1'].append(f1score)\n",
    "    state['hammingloss'].append(hammingloss)\n",
    "    state['val_microf1'].append(val_f1score)\n",
    "    state['epoch_time'].append(epoch_time)\n",
    "    state['val_hammingloss'].append(val_hamming)\n",
    "\n",
    "    state['optimizer'] = optimizer.state_dict()\n",
    "    state['last_model'] = model.state_dict()\n",
    "    \n",
    "\n",
    "    \n",
    "    if(best_train < f1score):\n",
    "      state['model_best_train'] = copy.deepcopy(model.state_dict())\n",
    "      best_train = f1score\n",
    "      state['best_train'] = best_train\n",
    "    \n",
    "    if(best_val < val_f1score):\n",
    "      state['model_best_val'] = copy.deepcopy(model.state_dict())\n",
    "      best_val = val_f1score\n",
    "      state['best_val'] = best_val\n",
    "\n",
    "    torch.save(state, save_path)\n",
    "    print('epoch:{} loss:{:.5f} hamming_loss:{:.5f} micro_f1score:{:.5f} val_hamming_loss:{:.5f} val_micro_f1score:{:.5f}'.\n",
    "          format(epoch, running_loss, hammingloss, f1score, val_hamming, val_f1score))\n",
    "    epoch+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14917f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word Representation Vector Model\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "WRVModel = loadWRVModel('./glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f27b4fa",
   "metadata": {},
   "source": [
    "# TEXT Preprocessing\n",
    "cleaning, and converting to sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7146c0b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/ssl/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CLEANING: The 3D pose estimation from a single image is a challenging problem due to\n",
      "depth ambiguity. One type of the previous methods lifts 2D joints, obtained by\n",
      "resorting to external 2D pose detectors, to the 3D space. However, this type of\n",
      "approaches discards the contextual information of images which are strong cues\n",
      "for 3D pose estimation. Meanwhile, some other methods predict the joints\n",
      "directly from monocular images but adopt a 2.5D output representation $P^{2.5D}\n",
      "= (u,v,z^{r}) $ where both $u$ and $v$ are in the image space but $z^{r}$ in\n",
      "root-relative 3D space. Thus, the ground-truth information (e.g., the depth of\n",
      "root joint from the camera) is normally utilized to transform the 2.5D output\n",
      "to the 3D space, which limits the applicability in practice. In this work, we\n",
      "propose a novel end-to-end framework that not only exploits the contextual\n",
      "information but also produces the output directly in the 3D space via cascaded\n",
      "dimension-lifting. Specifically, we decompose the task of lifting pose from 2D\n",
      "image space to 3D spatial space into several sequential sub-tasks, 1) kinematic\n",
      "skeletons \\& individual joints estimation in 2D space, 2) root-relative depth\n",
      "estimation, and 3) lifting to the 3D space, each of which employs direct\n",
      "supervisions and contextual image features to guide the learning process.\n",
      "Extensive experiments show that the proposed framework achieves\n",
      "state-of-the-art performance on two widely used 3D human pose datasets\n",
      "(Human3.6M, MuPoTS-3D).\n",
      "AFTER CLEANING: pose estimation single image challenging problem due depth ambiguity one type previous method lift joint obtained resorting external pose detector space however type approach discard contextual information image strong cue pose estimation meanwhile method predict joint directly monocular image adopt d output representation pd uvzr u v image space zr rootrelative space thus groundtruth information eg depth root joint camera normally utilized transform d output space limit applicability practice work propose novel endtoend framework exploit contextual information also produce output directly space via cascaded dimensionlifting specifically decompose task lifting pose image space spatial space several sequential subtasks kinematic skeleton individual joint estimation space rootrelative depth estimation lifting space employ direct supervision contextual image feature guide learning process extensive experiment show proposed framework achieves stateoftheart performance two widely used human pose datasets humanm mupotsd\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "preprocessed_text_train = [preprocessingText(text) for text in text_train]\n",
    "preprocessed_text_test = [preprocessingText(text) for text in text_test]\n",
    "\n",
    "print('BEFORE CLEANING: {}'.format(text_train[0]))\n",
    "print('AFTER CLEANING: {}'.format(preprocessed_text_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2326b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_text_train)\n",
    "sequences_text_train = tokenizer.texts_to_sequences(preprocessed_text_train)\n",
    "sequences_text_test = tokenizer.texts_to_sequences(preprocessed_text_test)\n",
    "\n",
    "X_train = torch.from_numpy(pad_sequences(sequences_text_train, maxlen=128))\n",
    "X_test = torch.from_numpy(pad_sequences(sequences_text_test, maxlen=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b4398c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE : 69418\n",
      "TOTAL OF UNKNOWN WORD : 44879\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = torch.zeros(VOCAB_SIZE, 300)\n",
    "\n",
    "unk = 0\n",
    "for i in range(1, VOCAB_SIZE):\n",
    "  word = tokenizer.index_word[i]\n",
    "  if word in WRVModel.keys():\n",
    "    embedding_matrix[i] = torch.from_numpy(WRVModel[word]).float()\n",
    "  else:\n",
    "    unk +=1\n",
    "print('VOCAB_SIZE : {}'.format(VOCAB_SIZE))\n",
    "print('TOTAL OF UNKNOWN WORD : {}'.format(unk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a6f0d",
   "metadata": {},
   "source": [
    "# Label Embedding\n",
    "\n",
    "Preparing Graph Attention Networks Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da6bcd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "label_embedding = torch.zeros(102,300)\n",
    "\n",
    "for index, label in enumerate(mlb.classes_):\n",
    "  words = label.split('-')\n",
    "  num_of_words = len(words)\n",
    "\n",
    "  for sublabel in words:\n",
    "    if sublabel in WRVModel.keys():\n",
    "      label_embedding[index] +=  torch.from_numpy(WRVModel[sublabel])\n",
    "  label_embedding[index] = label_embedding[index]/num_of_words\n",
    "\n",
    "print(label_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1176cba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "177c390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.x  = x\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b4660ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MAGNET(300, 150, adjacency, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e7a7733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 loss:17.15439 hamming_loss:0.02319 micro_f1score:0.00608 val_hamming_loss:0.01903 val_micro_f1score:0.00000\n",
      "epoch:2 loss:13.48097 hamming_loss:0.01918 micro_f1score:0.00000 val_hamming_loss:0.01903 val_micro_f1score:0.00000\n",
      "epoch:3 loss:13.36970 hamming_loss:0.01918 micro_f1score:0.00000 val_hamming_loss:0.01903 val_micro_f1score:0.00000\n",
      "epoch:4 loss:12.76321 hamming_loss:0.01918 micro_f1score:0.00000 val_hamming_loss:0.01903 val_micro_f1score:0.00000\n",
      "epoch:5 loss:10.11487 hamming_loss:0.01923 micro_f1score:0.00068 val_hamming_loss:0.01903 val_micro_f1score:0.00000\n",
      "epoch:6 loss:8.53562 hamming_loss:0.01908 micro_f1score:0.09124 val_hamming_loss:0.01898 val_micro_f1score:0.00691\n",
      "epoch:7 loss:7.36303 hamming_loss:0.01808 micro_f1score:0.30770 val_hamming_loss:0.01903 val_micro_f1score:0.00000\n",
      "epoch:8 loss:6.35366 hamming_loss:0.01351 micro_f1score:0.54764 val_hamming_loss:0.01068 val_micro_f1score:0.62892\n",
      "epoch:9 loss:5.75942 hamming_loss:0.01091 micro_f1score:0.65460 val_hamming_loss:0.01061 val_micro_f1score:0.62881\n",
      "epoch:10 loss:5.34520 hamming_loss:0.01014 micro_f1score:0.68242 val_hamming_loss:0.00967 val_micro_f1score:0.71002\n",
      "epoch:11 loss:5.13073 hamming_loss:0.00980 micro_f1score:0.69738 val_hamming_loss:0.00961 val_micro_f1score:0.69636\n",
      "epoch:12 loss:5.01661 hamming_loss:0.00971 micro_f1score:0.70149 val_hamming_loss:0.00955 val_micro_f1score:0.71509\n",
      "epoch:13 loss:4.90796 hamming_loss:0.00956 micro_f1score:0.70838 val_hamming_loss:0.00947 val_micro_f1score:0.70527\n",
      "epoch:14 loss:4.84972 hamming_loss:0.00948 micro_f1score:0.71077 val_hamming_loss:0.00944 val_micro_f1score:0.71383\n",
      "epoch:15 loss:4.79330 hamming_loss:0.00943 micro_f1score:0.71273 val_hamming_loss:0.00962 val_micro_f1score:0.68859\n",
      "epoch:16 loss:4.75881 hamming_loss:0.00936 micro_f1score:0.71557 val_hamming_loss:0.00935 val_micro_f1score:0.72363\n",
      "epoch:17 loss:4.65668 hamming_loss:0.00919 micro_f1score:0.72234 val_hamming_loss:0.00924 val_micro_f1score:0.72664\n",
      "epoch:18 loss:4.63009 hamming_loss:0.00920 micro_f1score:0.72260 val_hamming_loss:0.00934 val_micro_f1score:0.73212\n",
      "epoch:19 loss:4.57962 hamming_loss:0.00911 micro_f1score:0.72530 val_hamming_loss:0.00927 val_micro_f1score:0.71617\n",
      "epoch:20 loss:4.55098 hamming_loss:0.00899 micro_f1score:0.72967 val_hamming_loss:0.00992 val_micro_f1score:0.66452\n",
      "epoch:21 loss:4.50346 hamming_loss:0.00895 micro_f1score:0.73116 val_hamming_loss:0.00919 val_micro_f1score:0.71921\n",
      "epoch:22 loss:4.45515 hamming_loss:0.00888 micro_f1score:0.73350 val_hamming_loss:0.00914 val_micro_f1score:0.72480\n",
      "epoch:23 loss:4.42310 hamming_loss:0.00885 micro_f1score:0.73481 val_hamming_loss:0.00922 val_micro_f1score:0.72227\n",
      "epoch:24 loss:4.33009 hamming_loss:0.00867 micro_f1score:0.74105 val_hamming_loss:0.00915 val_micro_f1score:0.72354\n",
      "epoch:25 loss:4.27934 hamming_loss:0.00859 micro_f1score:0.74362 val_hamming_loss:0.00917 val_micro_f1score:0.72046\n",
      "epoch:26 loss:4.22744 hamming_loss:0.00848 micro_f1score:0.74684 val_hamming_loss:0.00928 val_micro_f1score:0.70844\n",
      "epoch:27 loss:4.15003 hamming_loss:0.00834 micro_f1score:0.75137 val_hamming_loss:0.00921 val_micro_f1score:0.71639\n",
      "epoch:28 loss:4.12958 hamming_loss:0.00834 micro_f1score:0.75142 val_hamming_loss:0.00908 val_micro_f1score:0.72905\n",
      "epoch:29 loss:4.06248 hamming_loss:0.00817 micro_f1score:0.75727 val_hamming_loss:0.00921 val_micro_f1score:0.71961\n",
      "epoch:30 loss:4.01696 hamming_loss:0.00813 micro_f1score:0.75880 val_hamming_loss:0.00898 val_micro_f1score:0.73206\n",
      "epoch:31 loss:4.02121 hamming_loss:0.00810 micro_f1score:0.76026 val_hamming_loss:0.00909 val_micro_f1score:0.72374\n",
      "epoch:32 loss:3.93085 hamming_loss:0.00800 micro_f1score:0.76365 val_hamming_loss:0.00889 val_micro_f1score:0.73718\n",
      "epoch:33 loss:3.87391 hamming_loss:0.00791 micro_f1score:0.76683 val_hamming_loss:0.00896 val_micro_f1score:0.73092\n",
      "epoch:34 loss:3.88579 hamming_loss:0.00786 micro_f1score:0.76845 val_hamming_loss:0.00900 val_micro_f1score:0.72253\n",
      "epoch:35 loss:3.78833 hamming_loss:0.00771 micro_f1score:0.77381 val_hamming_loss:0.00889 val_micro_f1score:0.73405\n",
      "epoch:36 loss:3.67484 hamming_loss:0.00754 micro_f1score:0.77912 val_hamming_loss:0.00893 val_micro_f1score:0.72658\n",
      "epoch:37 loss:3.68817 hamming_loss:0.00755 micro_f1score:0.77936 val_hamming_loss:0.00915 val_micro_f1score:0.71024\n",
      "epoch:38 loss:3.59827 hamming_loss:0.00735 micro_f1score:0.78526 val_hamming_loss:0.00873 val_micro_f1score:0.74214\n",
      "epoch:39 loss:7.20488 hamming_loss:0.01414 micro_f1score:0.52110 val_hamming_loss:0.00949 val_micro_f1score:0.71745\n",
      "epoch:40 loss:5.03524 hamming_loss:0.00921 micro_f1score:0.72047 val_hamming_loss:0.00911 val_micro_f1score:0.72560\n",
      "epoch:41 loss:4.42102 hamming_loss:0.00840 micro_f1score:0.74977 val_hamming_loss:0.00899 val_micro_f1score:0.73245\n",
      "epoch:42 loss:4.06412 hamming_loss:0.00790 micro_f1score:0.76702 val_hamming_loss:0.00903 val_micro_f1score:0.74462\n",
      "epoch:43 loss:3.90195 hamming_loss:0.00763 micro_f1score:0.77652 val_hamming_loss:0.00900 val_micro_f1score:0.72573\n",
      "epoch:44 loss:3.70018 hamming_loss:0.00742 micro_f1score:0.78369 val_hamming_loss:0.00897 val_micro_f1score:0.74635\n",
      "epoch:45 loss:3.61278 hamming_loss:0.00729 micro_f1score:0.78797 val_hamming_loss:0.00886 val_micro_f1score:0.74326\n",
      "epoch:46 loss:3.52364 hamming_loss:0.00717 micro_f1score:0.79221 val_hamming_loss:0.00875 val_micro_f1score:0.74169\n",
      "epoch:47 loss:3.47892 hamming_loss:0.00708 micro_f1score:0.79477 val_hamming_loss:0.00891 val_micro_f1score:0.74905\n",
      "epoch:48 loss:3.42642 hamming_loss:0.00699 micro_f1score:0.79788 val_hamming_loss:0.00890 val_micro_f1score:0.74290\n",
      "epoch:49 loss:3.38726 hamming_loss:0.00698 micro_f1score:0.79842 val_hamming_loss:0.00877 val_micro_f1score:0.75044\n",
      "epoch:50 loss:3.38008 hamming_loss:0.00688 micro_f1score:0.80168 val_hamming_loss:0.00886 val_micro_f1score:0.73055\n"
     ]
    }
   ],
   "source": [
    "train(model, X_train, X_test, label_embedding, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf3ba8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = (X_test, label_embedding)\n",
    "predictions = model.predict(X_test, label_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5faa2b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[11090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bb4b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "l = predictions.tolist()\n",
    "\n",
    "# aList = [41, 58, 63]\n",
    "jsonStr = json.dumps(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f49f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('pred.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(jsonStr, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0218b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_tf2",
   "language": "python",
   "name": "cuda_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
